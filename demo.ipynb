{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass Adam (and SGD) on a general problem class\n",
    "\n",
    "#### Notes\n",
    " \n",
    " - I super recommend using a GPU to run this, otherwise it's slow.\n",
    " - Code currently can't handle a non-diagonal data covariance for Adam. The \"gradient noise term\" isn't calculated correctly (It gets much more expensive). TODO\n",
    " - If Adam looks bad increase the \"history_length\" and \"num_samples\" values in the \"compute_phi...\" and \"compute_cov..\" functions. These control and approximation quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimizers import Adam, SGD, ResampledAdam\n",
    "from sdes import AdamSDE, SgdSDE\n",
    "from odes import AdamODE, SgdODE  \n",
    "\n",
    "def stats_off_diag(matrix):\n",
    "    n = matrix.shape[0]\n",
    "    mask = ~jnp.eye(n, dtype=bool)\n",
    "    return matrix[mask].mean(), matrix[mask].std()\n",
    "\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.numpy.linalg import norm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "# Setup the problem and parameters\n",
    "problem_type = 'linreg' # from  'logreg' or 'linreg', 'real_phase_ret'\n",
    "\n",
    "d = 500 # dimension\n",
    "m = 1 # number of classes (target dimension)\n",
    "beta1 = 0.3  # caution setting these values above 0.9 or so without increasing \"history_length\" \n",
    "beta2 = 0.3\n",
    "T = 10 # time to run SDE for\n",
    "\n",
    "lr = 0.7 # SDE learning rate\n",
    "lrk = lr / d # Optimzer learning rate\n",
    "\n",
    "# lr = lambda x: jnp.cos(2 * x / T) # SDE learning rate\n",
    "# lrk = lambda x: jnp.cos(2 * x / T / d) / d # Optimizer learning rate\n",
    "\n",
    "# cov = jnp.linspace(0.1, 1, d) # initialize with diagonal covariance\n",
    "cov = jnp.array([j**(-0.5) for j in range(1, d+1)])\n",
    "# cov = jnp.ones(d)\n",
    "\n",
    "# cov = jnp.ones(d)\n",
    "# U = scipy.stats.ortho_group.rvs(d)\n",
    "# cov = U @ jnp.diag(cov) @ U.T\n",
    "\n",
    "key = jax.random.PRNGKey(np.random.randint(0, 10000))\n",
    "key_init, key_opt, key = jax.random.split(key, 3)\n",
    "\n",
    "params0 = jax.random.normal(key_init, (d,m))\n",
    "# params0 = jnp.ones((d,m)) * 0.5\n",
    "# optimal_params = params0 + 0.1*jax.random.normal(key_opt, (d,m))\n",
    "optimal_params = jax.random.normal(key_opt, (d,m))\n",
    "\n",
    "# params0 = jnp.linspace(0.1, 1, d).reshape((d,1))\n",
    "# optimal_params = jnp.ones((d,m)) * 3\n",
    "\n",
    "\n",
    "\n",
    "params0 /= norm(params0, axis = 0)\n",
    "optimal_params /= norm(optimal_params, axis = 0)\n",
    "optimal_params *= 1\n",
    "\n",
    "\n",
    "# Run the optimizers\n",
    "adam = Adam(problem_type)\n",
    "resampled_adam = ResampledAdam(problem_type)\n",
    "sgd = SGD(problem_type)\n",
    "\n",
    "_, resampled_adam_risks = resampled_adam.run(params0, cov, T, lrk, optimal_params, beta1 = beta1, beta2 = beta2, eps = 0.00)\n",
    "_, sgd_risks = sgd.run(params0, cov, T, lrk, optimal_params)\n",
    "opt_p, adam_risks = adam.run(params0, cov, T, lrk, optimal_params, beta1 = beta1, beta2 = beta2, eps = 0.00)\n",
    "_, adam_risks_beta0 = adam.run(params0, cov, T, lrk, optimal_params, beta1 = 0, beta2 = beta2, eps = 0.00)\n",
    "\n",
    "# Run the SDE equivalents\n",
    "sgd_sde = SgdSDE(problem_type)\n",
    "adam_sde = AdamSDE(problem_type)\n",
    "\n",
    "# _, sgd_sde_risks, times_s = sgd_sde.run(params0, optimal_params, cov, T, lr)\n",
    "# params, adam_sde_risks, times_s = adam_sde.run(params0, optimal_params, cov, T, lr, beta1 = beta1, beta2 = beta2)\n",
    "\n",
    "# Run the ODE equivalents\n",
    "# adam_ode = AdamODE(problem_type)\n",
    "# sgd_ode = SgdODE(problem_type)\n",
    "\n",
    "# dt = 0.01\n",
    "# num_samples = 10000\n",
    "# sgd_risk, sgd_time, B = sgd_ode.run(params0, optimal_params, cov, T, lr, dt = dt * 2)\n",
    "# adam_risk, adam_time, B = adam_ode.run(params0, optimal_params, cov, T, lr, dt = dt, beta1 = beta1, beta2 = beta2, eps = 0.00, num_samples = num_samples)\n",
    "\n",
    "\n",
    "plt.yscale('log')\n",
    "# plt.xscale('log')\n",
    "plt.plot(adam_risks, label = 'Adam')\n",
    "plt.plot(resampled_adam_risks, label = 'Resampled Adam', ls = ':')\n",
    "plt.plot(adam_risks_beta0, label = 'Adam beta1=0')\n",
    "plt.plot(sgd_risks, label = 'SGD')\n",
    "\n",
    "# plt.plot(times_s * d, adam_sde_risks, label = 'Adam SDE')\n",
    "# plt.plot(times_s * d, sgd_sde_risks, label = 'SGD SDE')\n",
    "\n",
    "# plt.plot(adam_time * d, adam_risk, label = 'Adam ODE')\n",
    "# plt.plot(sgd_time * d, sgd_risk, label = 'SGD ODE')\n",
    "plt.title(f'Resampled Adam vs Adam vs SGD: lr: {lr}, d: {d}, beta1: {beta1}, beta2: {beta2}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from risks_and_discounts import grad_linreg, linreg_target\n",
    "from utils import make_data\n",
    "\n",
    "params = params0\n",
    "    \n",
    "history_length = 3\n",
    "d_vec1 = jnp.array([beta1**i for i in range(0, history_length)]) * (1-beta1)\n",
    "d_vec2 = jnp.array([beta2**i for i in range(0, history_length)]) * (1-beta2)\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "data = make_data(cov, subkey)\n",
    "target = linreg_target(data, optimal_params)\n",
    "current_grad = grad_linreg(params, data, target)\n",
    "\n",
    "\n",
    "second_mnts = []\n",
    "for l in range(history_length):\n",
    "    gradients = []\n",
    "    for _ in range(history_length):\n",
    "        key, subkey = jax.random.split(key)\n",
    "        data = make_data(cov, subkey)\n",
    "        target = linreg_target(data, optimal_params)\n",
    "\n",
    "        gradient = grad_linreg(params, data, target)\n",
    "        gradients.append(gradient)\n",
    "    gradients = jnp.array(gradients)\n",
    "    gradients = gradients.at[l,:,:].set(current_grad)\n",
    "    gradients2 = gradients**2\n",
    "    \n",
    "    second_mnt = jnp.sqrt(jnp.einsum('i,ijk->jk', d_vec2, gradients2))\n",
    "    second_mnts.append(second_mnt)\n",
    "second_mnts = jnp.array(second_mnts)    \n",
    "\n",
    "update = jnp.einsum('i,ijk->jk', d_vec1, (current_grad / second_mnts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1000, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(current_grad / second_mnts).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradients[l,:,:] = current_grad\n",
    "gradients = gradients.at[l,:,:].set(current_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.15970491]\n",
      " [-0.06021578]\n",
      " [ 0.1679486 ]]\n",
      "[[ 0.15970491]\n",
      " [-0.06021578]\n",
      " [ 0.1679486 ]]\n"
     ]
    }
   ],
   "source": [
    "print(gradients[l,:,:][0:3])\n",
    "print(current_grad[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phi: [ 2.3682418 -2.3690407]\n",
      "Condition number of Binv: 1.0671429634094238\n"
     ]
    }
   ],
   "source": [
    "# This compute the mean of the grad update\n",
    "\n",
    "from risks_and_discounts import f_linreg\n",
    "from utils import make_B\n",
    "\n",
    "\n",
    "n_samples = 200000\n",
    "params = params0 # the learned optimal parameters\n",
    "B = make_B(params, optimal_params,cov)\n",
    "f = f_linreg\n",
    "\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "key_Q, key_Q_hist, key_z, key_z_hist = jax.random.split(subkey, 4)\n",
    "Binv = jnp.linalg.inv(B)\n",
    "\n",
    "history_length = 250\n",
    "Q = jax.random.multivariate_normal(key_Q, mean = jnp.zeros(len(B)), cov = B, shape=(n_samples, 1))\n",
    "z = jax.random.normal(key_z, (n_samples,1))\n",
    "\n",
    "Q_history = jax.random.multivariate_normal(key_Q_hist, mean = jnp.zeros(len(B)), cov = B, shape=(n_samples, history_length))\n",
    "z_history = jax.random.normal(key_z_hist, (n_samples, history_length))\n",
    "\n",
    "decay_vec2 = jnp.array([beta2**i for i in range(1, history_length + 1)])\n",
    "decay_vec1 = jnp.array([beta1**i for i in range(1, history_length + 1)])\n",
    "\n",
    "fq = f(Q).squeeze(axis=1)\n",
    "Q = Q.squeeze()\n",
    "\n",
    "second_moment_history = jnp.einsum('abc,b->ac', f(Q_history)**2 * z_history[:,:,None]**2, decay_vec2)\n",
    "second_moment_average = jnp.sqrt((1-beta2) * (second_moment_history + z**2 * fq**2))\n",
    "\n",
    "first_moment_history = f(Q_history) * z_history[:,:,None]**2\n",
    "first_moment_history = first_moment_history / second_moment_average[:,None,:]\n",
    "first_moment_history = jnp.concatenate([first_moment_history,first_moment_history], axis = -1)\n",
    "first_moment_history_w_Q = first_moment_history * Q_history @ Binv\n",
    "\n",
    "\n",
    "current_avg = z**2 * fq / second_moment_average\n",
    "current_avg = (jnp.concatenate([current_avg,current_avg], axis = -1) * Q @ Binv).mean(axis=0)\n",
    "\n",
    "history_avg = jnp.einsum('abc,b->ac',first_moment_history_w_Q, decay_vec1).mean(axis=0)\n",
    "\n",
    "phi = (1-beta1) * (current_avg + history_avg)\n",
    "\n",
    "\n",
    "print(f'phi: {phi}')\n",
    "print(f'Condition number of Binv: {jnp.linalg.cond(Binv)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This well (eventually) compute the covariance\n",
    "key_Q, key_Q_hist, key_z, key_z_hist = jax.random.split(key, 4)\n",
    "\n",
    "history_length = 250\n",
    "Q = jax.random.multivariate_normal(key_Q, mean = jnp.zeros(len(B)), cov = B, shape=(n_samples, 1))\n",
    "z = jax.random.normal(key_z, (n_samples,1))\n",
    "\n",
    "Q_history = jax.random.multivariate_normal(key_Q_hist, mean = jnp.zeros(len(B)), cov = B, shape=(n_samples, history_length))\n",
    "z_history = jax.random.normal(key_z_hist, (n_samples, history_length))\n",
    "\n",
    "decay_vec1 = jnp.array([beta1**i for i in range(1, history_length + 1)])\n",
    "decay_vec2 = jnp.array([beta2**i for i in range(1, history_length + 1)])\n",
    "\n",
    "fq = f(Q).squeeze(axis=1)\n",
    "Q = Q.squeeze()\n",
    "\n",
    "second_moment_history = jnp.einsum('abc,b->ac', f(Q_history)**2 * z_history[:,:,None]**2, decay_vec2)\n",
    "second_moment_average = jnp.sqrt((1-beta2) * (second_moment_history + z**2 * fq**2))\n",
    "\n",
    "first_moment_history = f(Q_history) * z_history[:,:,None]\n",
    "first_moment_history = first_moment_history / second_moment_average[:,None,:]\n",
    "\n",
    "current_avg = z * fq / second_moment_average\n",
    "\n",
    "history_avg = jnp.einsum('abc,b->ac',first_moment_history, decay_vec1)\n",
    "\n",
    "vvv = (1-beta1) * (current_avg + history_avg)\n",
    "op = jnp.einsum('ab,ac->abc', vvv, vvv)\n",
    "op = op.mean(axis = 0)\n",
    "# op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op.shape\n",
    "# sqrtcov = jnp.linalg.cholesky(cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empirical_mean [-0.00091454]\n",
      "computed mean: -0.0018093104008585215\n",
      "--------------------\n",
      "avg diagonal of adam cov: 0.32484\n",
      "avg off diagonal: -1.9150359094055602e-06\n",
      "std off diagonal: 0.011296122334897518\n",
      "std diagonal: 0.0071851\n",
      "Theory cov = 0.3198 Id\n"
     ]
    }
   ],
   "source": [
    "from risks_and_discounts import grad_linreg, linreg_target\n",
    "from utils import make_data\n",
    "\n",
    "adam_ups = []\n",
    "n_samps = 1000\n",
    "hist_length = 50\n",
    "\n",
    "d_vec1 = jnp.array([beta1**i for i in range(0, hist_length)]) * (1-beta1)\n",
    "d_vec2 = jnp.array([beta2**i for i in range(0, hist_length)]) * (1-beta2)\n",
    "\n",
    "for _ in range(n_samps):\n",
    "    grad_hist = []\n",
    "    for _ in range(hist_length):\n",
    "        dat = make_data(cov)\n",
    "        target = linreg_target(optimal_params,dat)\n",
    "        grad = grad_linreg(params,dat,target)\n",
    "        grad_hist.append(grad)\n",
    "    grad_hist = jnp.array(grad_hist)\n",
    "    \n",
    "    m = jnp.einsum('a,abc->bc', d_vec1, grad_hist)\n",
    "    v = jnp.einsum('a,abc->bc', d_vec2, grad_hist**2)\n",
    "    \n",
    "    adam_up = m / jnp.sqrt(v)\n",
    "    adam_ups.append(adam_up)\n",
    "adam_ups = jnp.array(adam_ups)\n",
    "    \n",
    "adam_mean = np.mean(adam_ups,axis=0)\n",
    "\n",
    "adam_cov = jnp.einsum('abc,adc->abdc', adam_ups, adam_ups)\n",
    "adam_cov = jnp.mean(adam_cov,axis=0).squeeze()\n",
    "\n",
    "# check the mean adam update vs theory. this relies on params0 - optimal_params being a constant vector or near enough. Also uses identity covariance\n",
    "print(f'empirical_mean {adam_mean.mean(axis=0)}')\n",
    "print(f'computed mean: {(params0 - optimal_params).mean() * phi[0]}')\n",
    "\n",
    "print('-' * 20)\n",
    "\n",
    "m_of, std_of = stats_off_diag(adam_cov)\n",
    "print(f'avg diagonal of adam cov: {jnp.diag(adam_cov).mean():0.5}')\n",
    "print(f'avg off diagonal: {m_of}')\n",
    "print(f'std off diagonal: {std_of}')\n",
    "print(f'std diagonal: {jnp.diag(adam_cov).std():0.5}')\n",
    "\n",
    "print(f'Theory cov = {op.mean():0.5} Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
